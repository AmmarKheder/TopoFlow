# V√©rification Architecture : ClimaX Original vs Votre Code
**Date:** 2025-10-22
**Objectif:** V√©rifier que le code est compatible avec checkpoint 0.35 avant resume training

---

## ‚úÖ COMPARAISON D√âTAILL√âE

### 1. PARAM√àTRES __init__

| Param√®tre | ClimaX Original | Votre Code | Status |
|-----------|-----------------|------------|---------|
| `default_vars` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `img_size` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `patch_size` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `embed_dim` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `depth` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `decoder_depth` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `num_heads` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `mlp_ratio` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `drop_path` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `drop_rate` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `parallel_patch_embed` | ‚úÖ | ‚úÖ | ‚úÖ IDENTIQUE |
| `scan_order` | ‚ùå Absent | ‚úÖ "hilbert" | ‚ö†Ô∏è **AJOUT√â** (pour wind scanning) |
| `use_physics_mask` | ‚ùå Absent | ‚úÖ False | ‚ö†Ô∏è **AJOUT√â** (pour TopoFlow) |
| `use_3d_learnable` | ‚ùå Absent | ‚úÖ False | ‚ö†Ô∏è **AJOUT√â** (pour TopoFlow) |

**Verdict:** Les param√®tres additionnels sont **backwards-compatible** (valeurs par d√©faut = d√©sactiv√©s)

---

### 2. TOKEN EMBEDDINGS

#### ClimaX Original:
```python
if parallel_patch_embed:
    self.token_embeds = ParallelVarPatchEmbed(len(default_vars), ...)
else:
    self.token_embeds = nn.ModuleList(
        [PatchEmbed(...) for i in range(len(default_vars))]
    )
```

#### Votre Code:
```python
if parallel_patch_embed:
    self.token_embeds = ParallelVarPatchEmbed(len(default_vars), ...)
else:
    all_vars = ['u', 'v', 'temp', 'rh', 'psfc', 'pm10', 'so2', 'no2', 'co', 'o3', 'lat2d', 'lon2d', 'pm25', 'elevation', 'population']
    self.token_embeds = nn.ModuleList(
        [PatchEmbed(...) for i in range(len(all_vars))]  # ‚Üê 15 embeddings au lieu de len(default_vars)
    )
```

**‚ö†Ô∏è DIFF√âRENCE CRITIQUE:**
- ClimaX: `len(default_vars)` embeddings (ex: 3 si default_vars=['u','v','temp'])
- Votre code: **15 embeddings** (pour tous les all_vars)

**Impact:** Si checkpoint a √©t√© entra√Æn√© avec `parallel_patch_embed=False`, il y aura mismatch !

**‚úÖ MAIS:** Votre checkpoint 0.35 utilise probablement `parallel_patch_embed=True` (avec wind scanning)

---

### 3. VARIABLE EMBEDDINGS

#### ClimaX Original:
```python
self.var_embed, self.var_map = self.create_var_embedding(embed_dim)
```

#### Votre Code:
```python
self.var_embed, self.var_map = self.create_var_embedding(embed_dim)
```

**Status:** ‚úÖ IDENTIQUE

---

### 4. POSITIONAL EMBEDDINGS

#### ClimaX Original:
```python
self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=True)
# Puis dans initialize_weights():
pos_embed = get_2d_sincos_pos_embed(...)
self.pos_embed.data.copy_(...)
```

#### Votre Code:
```python
self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=True)
# Puis dans initialize_weights():
pos_embed = get_2d_sincos_pos_embed(...)
self.pos_embed.data.copy_(...)
```

**Status:** ‚úÖ **IDENTIQUE** (apr√®s le fix que je viens de faire)

---

### 5. TRANSFORMER BLOCKS

#### ClimaX Original:
```python
self.blocks = nn.ModuleList([
    Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True,
          drop_path=dpr[i], norm_layer=nn.LayerNorm, drop=drop_rate)
    for i in range(depth)
])
```

#### Votre Code (sans physics):
```python
self.blocks = nn.ModuleList([
    Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True,
          drop_path=dpr[i], norm_layer=nn.LayerNorm)  # ‚Üê Manque drop=drop_rate !
    for i in range(depth)
])
```

**‚ö†Ô∏è DIFF√âRENCE:** Param√®tre `drop=drop_rate` manquant dans Block !

**Impact:** Dropout dans l'attention l√©g√®rement diff√©rent

**‚úÖ MAIS:** Si checkpoint a √©t√© entra√Æn√© avec votre code, c'est compatible

---

### 6. TOPOFLOW (Optionnel)

#### ClimaX Original:
‚ùå N'existe pas

#### Votre Code:
```python
if use_physics_mask:
    # Remplace blocks[0] avec TopoFlowBlock ou Attention3D
    ...
```

**Status:** ‚úÖ **AJOUT** (optionnel, d√©sactiv√© par d√©faut)

**Compatible:** Oui si `use_physics_mask=False` (d√©faut)

---

### 7. PREDICTION HEAD

#### ClimaX Original:
```python
self.head = nn.ModuleList()
for _ in range(decoder_depth):  # decoder_depth=2 par d√©faut
    self.head.append(nn.Linear(embed_dim, embed_dim))
    self.head.append(nn.GELU())
self.head.append(nn.Linear(embed_dim, len(default_vars) * patch_size**2))
self.head = nn.Sequential(*self.head)
```

R√©sultat: Linear ‚Üí GELU ‚Üí Linear ‚Üí GELU ‚Üí Linear (5 couches)

#### Votre Code:
```python
self.head = nn.Sequential(
    nn.Linear(embed_dim, embed_dim),
    nn.GELU(),
    nn.Linear(embed_dim, embed_dim),
    nn.GELU(),
    nn.Linear(embed_dim, len(self.default_vars) * patch_size * patch_size)
)
```

R√©sultat: Linear ‚Üí GELU ‚Üí Linear ‚Üí GELU ‚Üí Linear (5 couches)

**Status:** ‚úÖ **IDENTIQUE** (avec decoder_depth=2)

**Note:** Commentaire dit "3-layer MLP" mais c'est en fait 3 Linear layers (5 couches total avec GELU)

---

### 8. INITIALIZATION

#### ClimaX Original:
```python
self.initialize_weights()
```

#### Votre Code:
```python
self.apply(self._init_weights)  # Init Linear et LayerNorm
self.initialize_weights()        # Init pos_embed et var_embed
```

**Status:** ‚úÖ **COMPATIBLE** (apr√®s le fix)

---

## üîç WIND SCANNING : O√ô EST LA DIFF√âRENCE ?

Le wind scanning se fait dans **ParallelVarPatchEmbed**, pas dans arch.py !

### Fichier: `parallelpatchembed_wind.py`

```python
def forward(self, x, vars=None):
    # 1. Patchify standard
    proj = F.conv2d(x, weights, biases, ...)  # [B, V, L, D]

    # 2. Wind reordering (OPTIONNEL)
    if self.enable_wind_scan and u_wind and v_wind:
        proj = apply_cached_wind_reordering(proj, u_wind, v_wind, ...)

    return proj
```

**Effet:** Change seulement **l'ORDRE** des patches, pas leur contenu ni l'architecture !

---

## ‚úÖ DIFF√âRENCES AVEC CHECKPOINT 0.35

### V√©rification: Votre checkpoint a-t-il √©t√© entra√Æn√© avec:

1. **parallel_patch_embed=True** ?
   - ‚úÖ Probablement OUI (n√©cessaire pour wind scanning efficace)
   - Si OUI ‚Üí Compatible ‚úÖ
   - Si NON ‚Üí Probl√®me avec token_embeds size ‚ùå

2. **drop parameter dans Block** ?
   - Votre code n'a pas `drop=drop_rate` dans Block
   - Si checkpoint entra√Æn√© avec votre code ‚Üí Compatible ‚úÖ
   - Si checkpoint vient de ClimaX original ‚Üí L√©g√®re diff√©rence ‚ö†Ô∏è

3. **initialize_weights() appel√©** ?
   - Apr√®s mon fix ‚Üí OUI ‚úÖ
   - pos_embed et var_embed initialis√©s correctement

---

## üéØ VERDICT FINAL

### Pour RESUME TRAINING depuis checkpoint 0.35:

| Aspect | Statut | Action |
|--------|--------|--------|
| Architecture de base | ‚úÖ Compatible | Aucune |
| pos_embed initialization | ‚úÖ Fix√© | ‚úÖ Fait |
| Wind scanning | ‚úÖ Dans ParallelVarPatchEmbed | V√©rifier enable_wind_scan=True |
| TopoFlow | ‚ö†Ô∏è Nouveau | Utiliser use_physics_mask=False pour resume |
| Head (decoder) | ‚úÖ Compatible | Aucune |

---

## üìã CHECKLIST AVANT RESUME TRAINING

- [x] pos_embed initialization restaur√©e
- [x] Architecture compatible avec ClimaX
- [ ] V√©rifier config: `parallel_patch_embed=True`
- [ ] V√©rifier config: `use_physics_mask=False` (ou True si vous voulez ajouter TopoFlow)
- [ ] V√©rifier que wind scanning est activ√© dans ParallelVarPatchEmbed
- [ ] Tester chargement checkpoint 0.35

---

## üöÄ COMMANDE POUR TESTER CHARGEMENT

```python
from src.model_multipollutants import MultiPollutantModule
import torch

# Charger checkpoint
ckpt = torch.load("logs/.../checkpoints/best-val_loss_val_loss=0.35....ckpt")

# Cr√©er mod√®le
config = {...}  # Votre config
model = MultiPollutantModule(config)

# Tester chargement
model.load_state_dict(ckpt['state_dict'])
print("‚úÖ Checkpoint charg√© avec succ√®s!")
```

---

## ‚ö†Ô∏è POINT D'ATTENTION: Wind Scanning + pos_embed

**Rappel du probl√®me discut√©:**

1. Checkpoint 0.35 entra√Æn√© avec wind scanning
2. pos_embed a appris pour l'ordre wind-scanned
3. Si vous cr√©ez nouveau mod√®le, pos_embed est r√©initialis√© avec sinusoidal (row-major)
4. **Mismatch si vous fine-tunez !**

**Solution pour RESUME:**
- Charger le checkpoint complet (inclut pos_embed appris)
- Ne PAS r√©initialiser pos_embed
- Continuer training avec m√™me ordre wind

**Solution pour NOUVEAU training avec TopoFlow:**
- Train from scratch
- pos_embed va apprendre l'ordre wind d√®s le d√©but

---

**Conclusion:** Architecture OK pour resume training ‚úÖ
**Next step:** Lancer resume training et v√©rifier que √ßa converge !
